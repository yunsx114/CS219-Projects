# CS219 Project 2 : 

# Computing the dot product of two vectors

## **目录**

1. **基本信息**
   - 1.1 实验目标
   - 1.2 测试环境
   - 1.3 代码结构
2. **方法实现与公平性**
   - 2.1 泛型与展开的选择
   - 2.2 数据结构优化（扁平化 vs 二维数组）
   - 2.3 时间测量与精度控制
   - 2.4 点乘结果保留
   - 2.5 代码实现
     - 2.5.1 java代码实现
     - 2.5.2 c代码实现
     - 2.5.3 随机样例生成实现
3. **性能对比实验**
   - 3.1 实验1:计算时间是否和向量维度，向量数量成线性正相关
   - 3.2 实验1:小数量级的向量数量和维度下速度的差异
   - 3.3 实验3:不同数据结构对速度的影响

4. **Java和C语言编译优化分析**
   - 4.1 O1优化
   - 4.2 O2优化
   - 4.3 O3优化
   - 4.4 Ofast优化
   - 4.5 Java优化分析
     - 4.5.1 JIT原理
     - 4.5.2 JIT加速效果

5. **多线程性能分析**
   - 5.1 OpenMP & Concurrent
   - 5.2 实验1:数据量较大时数据类型变化对多线程速度的影响
   - 5.3 实验2:向量数量增加下多线程的速度对比
   - 5.4 实验3:线程数量变化对多线程性能的影响

6. **结论与反思**

## 1 基本信息

### 1.1 实验目标

本实验旨在通过实现和对比C与Java语言的向量点积计算程序，在基于ARM64架构的测试环境下对比两种编程语言在不同种类的测试样例下计算性能、多线程效率的差异，并结合具体的底层汇编代码实现，从编译器优化、内存使用等角度分析差异的可能原因。具体实验目标如下：

1. 基础功能实现上，完成使用C和java两种语言实现支持多种数据类型的向量点乘计算，并采用相似的算法逻辑和数据结构、高精度的时间采样等方式来保证实验的公平性。此外，支持可变参数的随机样例生成，以提供多样化实验的基础
2. 性能对比实验上，通过脚本生成不同种类的测试样例并执行两种语言的代码进行计算速度对比，分析在不同参数变化下各个语言自身的速度差异和两者间的比较差异
3. 结合性能对比实验的结果，根据生成的汇编代码具体分析速度差异的原因，以及两种语言的编译器优化具体做了什么实现的性能提升
4. 实现两种语言的多线程版本，继续进行速度对比，并分析可能的原因
5. 进行总结分析，总结实验不足和可以改进的地方

### 1.2 测试环境

全部实验的测试环境基于一台Mac M1 pro，以下是部分硬件信息

| 模型名称   | MacBook Pro       |
| ---------- | ----------------- |
| CPU架构    | ARM64(aarch64)    |
| 芯片       | Apple M1 Pro      |
| 核总数：   | 8（6性能和2能效） |
| 内存       | 16GB              |
| 核心线程数 | 1                 |
| 主频       | 3.2GHz            |

### 1.3 项目结构

本次实验主要用到的代码包括：

1. python脚本：用于编译和生成汇编代码的`compile.py`，负责集成测试并生成图表的`test.py`
2. C代码：`CVecMulti.c`，实现了用C代码实现向量乘法，`CVecMultiThread.c`，实现了基于openMP的多线程版本
3. java代码：`javaVecMulti.java`，实现了java代码实现向量乘法。`javaVecMultiThread.java`，基于`java.util.concurrent`的多线程版本实现。此外，还有一个`RandomCaseGenerator.java`，负责根据给定的参数生成大量的随机样例，保存到项目根目录下的`randomcase.txt`文件中

由这些代码构成核心的项目可以大体表示为以下关联：

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-29 11.11.49.png" alt="截屏2025-03-29 11.11.49" style="zoom:50%;" />

​							 					*图1:项目结构示意图*

## 2 方法实现与公平性

### 2.1 泛型 or 展开

首先，为了保证java和c代码在向量乘法的实现上具有一定程度的公平性，我们要根据java和c语言在编译时的不同特性来确定实现的方案。其中，由于随机数的生成和测试时分开在多个文件中的，因此，在实现功能对接的时候为了实现多种数据类型的测试，必须要根据用于确定“给定的数据类型是什么”的参数`type`来确定具体用什么函数。

然而，由于函数实现的高度相似性，所以我们自然而然会想到用泛型来实现。但是这里必须警惕java和c语言泛型实现的不同性：java中的泛型在编译成字节码的过程中会被擦除，变成`object`类或者边界类，再通过强制转换实现类型的多样变化，因此具有一定的额外开销；而c语言并没有直接的泛型可以使用，但可以通过`void*` 指针或者宏定义(`_Generic` 编译后也类似宏展开)来实现类似泛型的效果。宏展开会增加代码量，但对性能开销的影响很低，而`void*` 类似java的实现效果需要类型转换，因此对时间开销有影响，而且安全性较低。

因此，本着对最优时间开销的追求，我们选择对每一个数据类型设置一个单独函数来实现。

### 2.2 数据结构优化（扁平化 vs 二维数组）

其次，计算开销中最大的部分在于访问向量每一维所在的地址并取出数据进行计算，如果采用二维数组存储，其中一个内部数组代表一个`dim`维向量，则对于c来说，本质上是一个二重指针，需要不断解引用，增加额外开销；对于java来说，需要多次边界检查，而且内存空间不一定连续，也会降低性能。因此，我们采用扁平化的一维数组存储向量数据，保证内存连续，减少各种开销

### 2.3 时间测量与精度控制

由于要测量小样本量的数据，我们对两个代码都要采用精度到纳秒级别的时钟。java代码的时间计算利用`System.nanoTime()`，而c代码利用构造体`timespec`，以函数`clock_gettime(CLOCK_MONOTONIC, &t)`来获取单调时钟的秒和纳秒级时间。此外，两种测量方式均基于系统时钟，不会出现系统时钟和挂钟混用导致的不一致性。最后，由于调用时钟本身也具备一定的时间，所以实际测量的是计算总耗时加调用计算结束时间的函数的时间，不过这一部分时间大致不到`50ns`，而且两个代码都会同步增加这一部分的开销，因此暂时忽略。

此外，为了降低测试的不稳定性和单次测试的结果偶然性，我设置了一个`CYCLE_TIME`参数，读入后多次计算，最终得到平均值。本次实验中该参数的值为`100`

### 2.4 点乘结果保留

本次实验主要集中于向量的点乘时间开销的对比，因此按道理是不需要计算出结果的，因为计算的样例是随机的没有实际的意义。然而，在测试中我发现，c语言的O2和O3优化会针对没有用到结果的计算进行激进优化，会把整个计算的循环删除，导致最后的时间计算为`0`，因此，我在java和c中均保留了向量点乘的结果并打印输出，这其实是有意义的，因为实际应用中我们肯定会用到向量点乘的结果。

### 2.5 代码实现

#### 2.5.1 java代码实现

以`int`类型为例，先读入数据，再开始时间计算。进入向量计算部分后，每相邻`dim`个数为一个向量的数据，每两个大小为`dim`的数做一次向量点乘，总共`N`个数据，因此数据同一测试循环内不重复利用，仅计算一次。

```java
//part of code in main
case 1 -> { // int
    int[] intData = new int[N * dim];
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < dim; j++) {
            intData[i * dim + j] = sc.nextInt();
        }
    }
    for (int i = 0; i < CYCLE_TIME; i++) {
        long start = System.nanoTime();
        sum = calculateVectorInt(intData, N, dim);
        totalTime += System.nanoTime() - start;
    }
}
//implementation of int vector calculation
public static int calculateVectorInt(int[] flatData, int N, int dim) {
  int sum = 0;
  for (int i = 1; 2 * i <= N; i++) {
      int offset1 = (2 * i - 2) * dim;
      int offset2 = (2 * i - 1) * dim;
      for (int j = 0; j < dim; j++) {
          sum += flatData[offset1 + j] * flatData[offset2 + j];
      }
  }
  return sum;
}
```

#### 2.5.2 C代码实现

以`int`类型为例，其余部份实现和java基本相同

```c
//part of code in main
case 1: { // int
    data = malloc(N * dim * sizeof(int)); //free is not shown here
    int* flat_data = (int*)data;
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < dim; j++) {
            if (fscanf(file, "%d", &flat_data[i * dim + j]) != 1) {
                printf("parse number failed\n");
                goto cleanup;
            }
        }
    }
    for (int i = 0; i < CYCLE_TIME; i++) {
        clock_gettime(CLOCK_MONOTONIC, &start);
        sum = calculate_vector_int(flat_data, N, dim);
        clock_gettime(CLOCK_MONOTONIC, &end);
        avg_interval += time_diff(&start, &end);
    }
    avg_interval /= CYCLE_TIME;
    break;
}
//implementation of int vector calculation
int calculate_vector_int(int* data, int N, int dim) {
    int sum = 0;
    if (!data) {//null pointer detection
        printf("empty data passing\n");
        return sum;
    }
    for (int i = 1; 2 * i <= N; i++) {
        int offset1 = (2 * i - 2) * dim;
        int offset2 = (2 * i - 1) * dim;
        for (int j = 0; j < dim; j++) {
            sum += data[offset1 + j] * data[offset2 + j];
        }
    }
    return sum;
}
```

#### 2.5.3 随机样例生成实现

随机样例实现采用java的`Random()`函数，有几个基本的参数：

```java
int type;               // 生成类型：1=int, 2=short, 3=signed char, 4=double, 5=float
boolean is_simple;      // 值范围：简单值（较小）或复杂值（较大）
boolean std;            // 对于float/double，是否生成标准范围(0-1)
int dimension;          // 向量维度
int N;                  // 样本数量 N = 数量 N/2 = 计算次数
```

其中，当`is_simple` 为 `true`时，全部种类的样例范围均保持在`unsigned char`的范围内(-128~127)，当`std`为`true`时，`float`和`double`的全部样例范围保持在(0,1)

## 3 **性能对比实验**

### 3.1 实验1:计算时间是否和向量维度，向量数量成线性正相关

在比较java和c代码的运行效率之前，我们有必要证明实验本身最基础的理论和实践一致性，保证后续实验是可靠的。两个维度为m的向量点乘，时间复杂度易知为O(2m)=O(m)，则n次向量点乘的总时间复杂度为O(n*m)，因此，我们可以从多个角度证明：

- 如果我们只改变维度m，不改变向量数量n；或者只改变向量数量n，不改变维度m，那么时间应该会呈现线性增长关系
- 如果n*m的值不变，改变n和m的比例，那么时间应该保持大体一致
- 如果同时增加m和n的值，那么总时间会呈现导数大于0的曲线增长关系

实际实验中，我们采用的执行文件包括五个：java乘法的class文件，c乘法在无优化(O0)、O1、O2、O3编译优化后得到的可执行文件。使用的数据类型为`float`；`is_simple=false`，因此数据的位数较高。最后得到如下图线：

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-28 18.05.55.png" alt="截屏2025-03-28 18.05.55" style="zoom:30%;" />

​										*图2:向量维度对计算时间的影响*

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-28 18.08.26.png" alt="截屏2025-03-28 18.08.26" style="zoom:30%;" />

​										*图3:向量数量对计算时间的影响*

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-28 18.10.03.png" alt="截屏2025-03-28 18.10.03" style="zoom:30%;" />

​							 	*图4:向量数量和维度总数不变对计算时间的影响*

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-28 18.09.24.png" alt="截屏2025-03-28 18.09.24" style="zoom:30%;" />

​								*图5:向量数量和维度同步增长对计算时间的影响*

以上四张图分别是：

1. n保持`10000`，维度m增长时的时间开销变化，呈现明显的线性增长关系
2. m保持`5000`，数量n增长时的时间开销变化，呈现明显的线性增长关系
3. m*n保持`50000000`，n/m的值发生变化的时间开销变化，呈现为一条水平直线
4. m和n同步增长下的时间开销变化，呈现为类似二次方曲线的的增长关系

由此可以发现，四项实验的结果均符合预期，实验最基本的运行是可靠的。

然而，除开各自的变化符合理论预期之外，我们可以发现，这四张图总体都表明，在当前m和n的数量级下，五种文件的计算速度均保持了稳定的相互大小关系，即：无编译优化下的c代码计算速度最慢，O1优化后的c代码有了较大幅度的速度提升，而java、O2、O3优化的c代码图线重合程度较高，整体计算速度在各个采样点都基本一致，比起O1优化的c代码又有了小幅度的速度提升。这个实验结果让我们发现，在当前实验采用的数据数量级下，java的计算速度居然和最高编译优化级别的c代码差不多，c代码真的执行速度比java要快吗？

### 3.2 实验2:降低向量数量和维度的变化

于是，我们试着先降低向量维度和数量，再执行一次实验，看看会得到什么结果：

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-28 19.04.24.png" alt="截屏2025-03-28 19.04.24" style="zoom:30%;" />

​										*图6:向量维度对计算时间的影响(数据量小)*

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-28 19.08.39.png" alt="截屏2025-03-28 19.08.39" style="zoom:30%;" />

​										*图7:向量数量对计算时间的影响(数据量小)*

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-28 19.09.41.png" alt="截屏2025-03-28 19.09.41" style="zoom:30%;" />

​							 	*图8:向量数量和维度总数不变对计算时间的影响(数据量小)*

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-28 19.10.37.png" alt="截屏2025-03-28 19.10.37" style="zoom:30%;" />

​							 	*图9:向量数量和维度同步增长对计算时间的影响(数据量小)*

得到的以上四张图，对应的实验不变，n的基本值从`10000`改成100，m的基本值从`5000`改成`50`。

在数据数量级改变后，有趣的事情发生了：根据前两张图的趋势变化，我们发现随着数据大小的增长，最慢的仍是无优化的c代码，其次为java，之后分别是O1、O2、O3编译优化的c代码。然而，在数据量最小的采样点，java的图线增长缓慢，且起始点较高，也因此成为了最慢的代码，第三张图上这一点很明显，保持n*m=`5000`（最小）时，java代表的蓝色图线一直处于最高处。在后续的增长过程中速度也没有超过c的O1编译优化。

也就是说，由实验1和2我们可以初步发现，在使用float类型进行向量计算时，java的实际计算执行速度是随着数据量的增长而逐渐变快的，从一开始的比无优化的c代码更慢，到变成接近O3优化的c代码的速度。

### 3.3 实验3:不同数据结构对速度的影响

以上结论是使用`float`类型得到的结论，那么，接下来我们测试不同数据结构对计算速度的影响

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-28 19.50.19.png" alt="截屏2025-03-28 19.50.19" style="zoom:30%;" />

​							 	*图9:不同数据结构对计算速度的影响*(数据量小)

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-28 19.50.53.png" alt="截屏2025-03-28 19.50.53" style="zoom:30%;" />

​							 	*图10:不同数据结构对计算速度的影响*(数据量大)

上图中，type的数字和具体代表的数据类型的关系：1=int, 2=short, 3=signed char, 4=double, 5=float

第一张图是`m=50`，`n=100`情况下，五种执行文件在各个数据类型下的表现；第二张图是`m=5000`，`n=10000`情况下，五种执行文件在各个数据类型下的表现。

仅看第一张图，在数据数量级较小的情况下，可以发现如下规律：

- 首先，java在所有数据类型下最终的执行时间相似，也就是说，数据类型对实际的执行时间影响较小，且开销显著大于c代码
- 其次，对于c代码，可以很明显发现，**在数据结构为浮点类型时的整体时间开销大于整型的时间开销**

对于O1,O2,O3编译优化之间的微弱差异，之后我又进行了几次测试，发现每次结果都比较随机，可能更高的优化反而更慢，也可能更快或者保持一致，所以这里实际的执行时间可能受到计算机实际执行时的状态而有所差异，我们暂且不做比较

再看第二张图，在数据量较大的情况下，会发现更多明显且稳定的规律：

- 第一，在较高的数据量加持下，java的时间开销终于降下来了，并且和编译优化后的c代码在一个级别，而无优化的c代码最慢

- 第二，**对于全部数据类型，优化后的c代码中O1最慢，O2和O3的速度比O1更快，但二者基本速度一致，O3优化没有实际的体现**

- 第三，**在数据类型为int，float，double时，java的执行速度和O2、O3的速度基本持平**

- 第四，上承第二、三点，我们发现**对于short和signed char类型，O2和O3的相对速度显著提高，时间开销很低**


根据以上信息和规律，我产生了以下疑问：

1. 总的来讲，O1、O2、O3的优化到底一步一步做了什么，为什么O3优化没有显著提升？java的优化又做了什么才能和c语言的编译优化持平？
2. 为什么java在数据量较小时执行速度最慢，且受到数据类型的影响较小，而数据量增大后，计算速度逐渐加快，最后在部分情况下和最高优化的c代码的计算速度持平？
3. 基于对第一点的进一步追问，为什么O2和O3优化可以大幅缩小short和unsigned char的实际计算速度？

## 4 Java和C语言的编译优化分析

c代码在经过编译后会形成对应于测试环境的系统架构的汇编代码，再转换成机器码，但是gcc编译器在编译阶段提供了多层优化方式来提高生产的汇编代码的实际内容以提高效率，为了理解编译究竟做了什么优化，我们可以根据gcc生成的.o文件反汇编得到对应的汇编代码，然后对各层优化级别的代码进行比较

```c
//以生成O1优化为例，加上-g生成调试信息，以在汇编代码处标记源代码，最后生成obj文件
gcc -g -O1 -c CVecMulti.c out/c_obj-O1.o
//用objdump反汇编，得到基于硬件系统的指令集的汇编代码
objdump -S out/c_obj-O1.o > c_ass-O1.s
//用diff在控制台比较两种优化级别代码的代码，用ansi2html工具以html格式保存
diff -y --width=$COLUMNS out/c_ass-O0.s out/c_ass-O1.s | colordiff | ansi2html > diff_output.html
```

### 4.1 O1优化

我们以c代码中的`calculate_vector_int()`方法为例，对比O0和O1汇编代码的异同。以下左侧为O0汇编代码，右侧为O1汇编代码，由于aarch64架构的汇编指令我没有学过，因此以下分析借助了deepseek的关键内容提取和解释：

![截屏2025-03-29 11.41.06](/Users/yun/Desktop/C_project2/截屏2025-03-29 11.41.06.png)

首先，我们可以看到O0汇编代码预先在栈中分配了`0x50`,也就是`80`字节的空间，之后向栈中显式存入了所有输入进来的数据，也就是`data`，`N`，和`dim`三个参数。因为它不相信寄存器中的值会被安全保留，所以向栈中保存了自己要用的参数，之后全部的涉及传入的参数的操作都需要从栈上重新获取参数的值。而O1优化仅分配了`16`字节，而且**没有使用栈来管理传入的参数，而是尽量使用寄存器。**

![截屏2025-03-29 11.41.35](/Users/yun/Desktop/C_project2/截屏2025-03-29 11.41.35.png)

在判断`data`是否为`null`时，无优化的代码进行了大量检查，标志位设置，和跳转分支。而O1优化则**直接用`cbz`一条指令**看`data`是否为`null`，如果是，则跳到下一个函数定义的位置。这其中的代价我们也看得出来，根据标红的区域，O1优化直接删除了`printf`的部分，也就是说，虽然O1的优化可以正常检查`data`是否为null并跳出函数，但是不会在控制台打印报错信息。

![截屏2025-03-29 11.42.08](/Users/yun/Desktop/C_project2/截屏2025-03-29 11.42.08.png)

到了最重要的向量乘法环节，对于第一个for循环`for (int i= 1; 2*i <= N; i++)`，**汇编代码所做的是将循环条件中的i和N存入栈中，再拿出来做比较最后设置跳转；而O1却全程没有采用栈写入和读出，i和N都是储存在寄存器中的**。

此外，我们可以看到O1删除了O0中红色部分的内容，也就是两个`int * vec = &data[(2 * i-2)* dim]`，但是c源代码中这一句是`int offset = (2 * i - 2) * dim;`，也就是说，实际编译的过程中，编译器将我预先定义的offset值转化成了指向该位置的指针。但是O1优化就没有这么做，它直接删除了红色字段的部分，包括两条offset的计算。

由图可知，vec1和vec2的计算是十分相似的，而且仅就值而言，只差了一个`dim`的大小，但O0却完全没有利用这一点，而O1优化中编译器发现，vec1的地址可以表示为`vec1 = data + (2*i-2)*dim;`，因此O1只计算 `vec1` 的基地址，`vec2` 可以通过 `vec1 + dim` 直接访问。而且由于 `i` 每次递增 `1`，`vec1` 的步长是 `2*dim`（`(2*(i+1)-2)*dim - (2*i-2)*dim = 2*dim`，因此，优化后的代码直接维护一个指针 `x0`，每次循环增加 `dim * 8`(是步长的四倍，因为int是四字节)。这样O1就只需要计算好`vec1`的初始地址存到`x15`，之后用`vec1+dim`算出`vec2`的值(`dim`存在`x9`寄存器)，再不断维护`x0`使之每次循环递增 `dim * 8`即可

![截屏2025-03-29 11.52.39](/Users/yun/Desktop/C_project2/截屏2025-03-29 11.52.39.png)

这样，在计算部分，O1优化所做的就仅仅是：

- 取出当前`vec1`的值(第一句)，
- 取出vec1的值并加上`dim*4`得到vec2(第二句)，
- 最后利用`madd`指令一次性完成向量相乘和累加到`sum`的两步操作(第三句)

而O0还要不断从栈中取出和写入值，采用传统的mul指令做计算，降低了计算的速度。

**综上所述，O0到O1的优化主要发生在大幅度减少栈读写和空间分配，尽可能使用寄存器来完成计算；对于一些复杂的跳转分支进行简化；对于高相似度的代码进行合并和优化，并且十分智能的针对一些计算找到其内部规律，并根据规律优化计算方式，避免循环内重复计算。**

### 4.2 O2优化

 在对比O1和O2上，我们直接看计算向量点乘的部分：

![截屏2025-03-29 14.21.06](/Users/yun/Desktop/C_project2/截屏2025-03-29 14.21.06.png)

相比于左侧的O1，右侧O2的优化反而多了几行，而且使用的指令也不一样了，O1使用的是`ldr`，而O2用的是`ldp`，此外，O2还用到了4次叫`mla.4s`的指令。根据deepseek的解释，这里O2的优化逻辑如下：

```c
d7c: ad7f1504      ldp     q4, q5, [x8, #-0x20]   ; 加载 vec1 的 4个 int 到 q4,q5 (128位)
d80: ad401d06      ldp     q6, q7, [x8]           ; 加载 vec1 的另外 4个 int 到 q6,q7
d84: 8b0c0101      add     x1, x8, x12            ; 计算 vec2 的地址
d88: ad7f4430      ldp     q16, q17, [x1, #-0x20] ; 加载 vec2 的 4个 int 到 q16,q17
d8c: ad404c32      ldp     q18, q19, [x1]         ; 加载 vec2 的另外 4个 int 到 q18,q19
d90: 4ea49601      mla.4s  v1, v16, v4            ; 向量乘加: v1 += v16 * v4 (4个 int)
d94: 4ea59620      mla.4s  v0, v17, v5            ; 向量乘加: v0 += v17 * v5
d98: 4ea69642      mla.4s  v2, v18, v6            ; 向量乘加: v2 += v18 * v6
d9c: 4ea79663      mla.4s  v3, v19, v7            ; 向量乘加: v3 += v19 * v7
```

其中，**前面的部分不再是一次只加载向量的一个值，而是同时加载8个值；而后面的mla.4s指令则是利用了NEON寄存器的SIMD并行计算，也就是说，这里同时计算四个int的向量点乘，吞吐量提升了四倍。**

这里提到的NEON寄存器是aarch64架构的指令集扩展，包括专用的32个128位寄存器，和各种支持整数/浮点数据的并行加减乘除等操作的指令，SIMD(Single Instruction, Multiple Data)是一种并行计算的技术，可以同时将多个数据打包，最后用一条指令完成计算。这里的并行计算是基于一个核心的，因此本质上不属于多线程，而仅仅是利用宽寄存器完成了单指令多数据的并行计算。

这样做的好处不仅仅在于提高了计算效率，也在于减少边界检查的次数

![截屏2025-03-29 14.44.26](/Users/yun/Desktop/C_project2/截屏2025-03-29 14.44.26.png)

正如上图所示，虽然O2优化在内循环中多了很多内容，但是其中包含了两段分支(图中绿色部分多了一个for循环)，即根据dim是否小于16来决定是否开启SIMD并行计算，如果是小数量的计算，则不采用并行计算；如果是较多数量的计算，则采用并行计算，其中，d70行的x17负责控制循环次数，而因为这里又是同时加载4*4=`16`个int数，所以x17的自增很快，这样就大量降低了循环条件的判定，从而降低开销。

此外还有一个问题，那就是针对于为什么O2和O3相比于O1计算short和byte类型的向量比计算int类型向量有着更显著的速度提升？我们分别看看O2下计算short和unsigned char类型的函数：

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-29 15.05.59.png" alt="截屏2025-03-29 15.05.59" style="zoom:50%;" />

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-29 15.06.31.png" alt="截屏2025-03-29 15.06.31" style="zoom:44%;" />

和int类型的处理相似也有不同，short类型采用的SIMD指令是`smlal.4s`和`smlal2.4s`指令，是针对short类型的特殊处理，但是我们看到int只有重复4行，而short重复了8行，也就是说，int类型可以一次性处理4个int，而short类型甚至可以做到一次性处理8个。吞吐量又是int的一倍。这其中的道理在于，128位的宽寄存器可以正好存下4个int(4 * 32=128)，或者8个short(8 * 16=128)。

而下面一张图的byte，利用`sshll2.8h`和`sshll.8h`指令将两个byte数合并成一个short类型，也就是从8位扩展到16位，(查找文档后，发现NEON中没有支持直接乘加byte类型的指令，所以只能进行位数扩展后才能计算)，之后的步骤同理，也是smlal和smlal2重复总共8次，虽然并行化计算仍然是8个16字节的数，但是每一个数本身又是两个数，所以实际并行计算的是16个数。这样就又比short的吞吐量更大一倍。实际实验中，由先前的图表所示，unsigned char类型的计算确实比short要快整整一倍左右。

### 4.3 O3优化

O3优化相比O2，在本次使用的代码中所做的优化较不明显，让我们分析汇编层面它实际做的工作：

![截屏2025-03-29 18.21.44](/Users/yun/Desktop/C_project2/截屏2025-03-29 18.21.44.png)

将部分循环内条件判定放到循环外：O2中的d28行原本在循环内，对应到O3中的f0c行放到了循环外，降低开销

![截屏2025-03-29 18.25.38](/Users/yun/Desktop/C_project2/截屏2025-03-29 18.25.38.png)

采用`b.hs`代替`b.lo`，针对高频执行的循环，可能会提取前几次迭代的过程预先单独执行，以保证字节对齐，并且更有可能被别的优化策略识别到。

![截屏2025-03-29 18.35.51](/Users/yun/Desktop/C_project2/截屏2025-03-29 18.35.51.png)

用lsl预先计算步长(x11是`dim`，通过位数左移得到`dim` * 8 存入x15)

**此外，还大幅更改了一些编译代码的顺序，但是实际逻辑变动不大。O3的优化实际上可以实现小函数内联，消除公共子表达式，但是由于本代码中缺少小型函数，也避免了子表达式的使用，所以实际计算提升空间较小，表现为在图表中和O2保持几乎相同的性能表现**

### 4.4 Ofast优化

在调查各个编译器优化级别的文档时，我发现了编译器还提供了一个Ofast的优化级别，所以我将其加入实验，重新测试数据类型变化时的性能表现，得到如下图：

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-29 18.42.21.png" alt="截屏2025-03-29 18.42.21" style="zoom:40%;" />

发现Ofast在计算浮点类型的数据时相比O2和O3还用java有很大幅度的性能提升，所以我们来看看它的编译优化是这么执行的

![截屏2025-03-29 18.53.04](/Users/yun/Desktop/C_project2/截屏2025-03-29 18.53.04.png)

在计算向量点乘的部分，虽然O3采用了`fmul.4s`来进行SIMD并行化计算，但是每次都要通过无法并行化的`fadd`进行结果的累加，**而Ofast直接通过`fmla.4s`一个指令完成了乘法和累加，大幅缩减了代码量和计算开销。**

### 4.5 Java编译优化分析

#### 4.5.1 JIT原理

说完了c代码的编译优化是如何提高可执行文件最后的计算性能的之后，我们还需要调查java是怎么提高它的代码效率，使得它在数据较大的情况下计算速度可以达到和高编译优化级别的c代码一个水平，又是什么造成它在计算较小的数据量时性能低下的。

想要理解java的加速是怎么发生的，我们自然要明白java编译到运行代码背后的流程。首先，java会将写好代码的.java文件编译成JVM可以识别解析的字节码(ByteCode)文件(后缀为.class)，这也是java可以做到跨平台语言的根基之一，也就是不同架构的平台可以执行同一份字节码文件，也可以针对一份.java文件生成极其类似的字节码文件，实现"Write once, Run anywhere"。

**当我们启动程序之后，JVM会运行对应的字节码文件并且逐句解释并执行，因为不是整体一次性全部编译并统一执行，所以实际执行速度较为缓慢。因此java引入了一个运行时的编译器JIT(Just in time)，当JVM开始逐句解释代码时，还会实时监控栈顶调用的方法，当某个方法被实际调用次数超过阈值之后，JIT会将这部分代码编译成对应架构下的机器码，在下次调用时直接执行机器码，跳过重复的解释，也减少代码缓存。**

其中，由于历史原因，JIT也分成C1和C2两种优化，前者称为客户端编译器，可以快速生成有效代码并做一些简单的优化，比如函数内联等等；后者称为服务端编译器，针对长期运行的程序做深度优化。JIT实际编译时，会根据不同的阈值混合调用这两种编译器，以达成更好的效果。

#### 4.5.2 JIT加速效果

了解了JIT的运行原理，我们可以通过配置java虚拟机的运行选项来生成编译和执行的日志文件，再用JITwatcher工具来查看源代码对应的字节码和经过JIT编译后的机器码所对应的架构下的汇编代码，进而分析它的实际优化是如何实现的。

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-29 20.05.37.png" alt="截屏2025-03-29 20.05.37" style="zoom:30%;" />

对于一份采用float为基础数据类型的样例，我们可以从软件的UI中看到，左侧为java源代码，中间为字节码，右侧为根据机器码翻译回本地架构的汇编代码。在点击计算其他数据类型的函数时，汇编处没有显示，这说明这些代码并没有被大量调用，也就没有被JIT识别成热点代码并被编译，这也确实符合运行事实。下面我们根据汇编代码看看JIT做了哪些优化

```
0x00000001141c9b6c: add x12, x1, x12, lsl #2         ; &data[p2+j]
0x00000001141c9b70: ldr s18, [x12, #16]              ; 加载data[p2+j]
0x00000001141c9b74: ldr s17, [x12, #20]              ; 加载data[p2+j+1]
0x00000001141c9b78: ldr s22, [x12, #24]              ; 加载data[p2+j+2]
0x00000001141c9b7c: add x14, x1, x14, lsl #2         ; &data[p1+j]
0x00000001141c9b80: ldr s16, [x14, #16]              ; 加载data[p1+j]
0x00000001141c9b84: ldr s20, [x14, #20]              ; 加载data[p1+j+1]
0x00000001141c9b88: ldr s19, [x14, #24]              ; 加载data[p1+j+2]
......
add w15, w15, #0x4   ; j += 4（步长4）
```

**可以看到，主要的加载部分发生在寄存器之间（s系列是NEON的寄存器），几乎没有栈读写，仅这一点就完成了O1主要做的优化之一**

**此外，这一部分的运算也是采用了在一个循环内4个float数一起加载，同步展开和并行运算，降低了循环次数，提高了步长，做到了O2主要提供的优化方案。然而jit并没有采用fmla指令，导致java在float和double运算上虽然和O2、O3持平，但是没有达到Ofast的层次**

```
0x00000001141c9ad8: ldr w12, [x1, #12]               ; 加载数组长度
0x00000001141c9adc: sxtw x7, w5                      ; 扩展p2
0x00000001141c9ae0: sxtw x6, w0                      ; 扩展p1
0x00000001141c9ae4: add x15, x7, x4                  ; p2 + dim
0x00000001141c9ae8: add x14, x6, x4                  ; p1 + dim
0x00000001141c9aec: sxtw x16, w12                    ; 扩展数组长度
0x00000001141c9af0: sub x15, x15, #0x1               ; 计算上界
0x00000001141c9af4: sub x14, x14, #0x1               ; 计算上界
0x00000001141c9af8: cmp w5, w12                      ; 边界检查(p2)
0x00000001141c9afc: b.hs <range_check_fail>          ; 越界跳转
0x00000001141c9b00: cmp x15, x16                     ; 边界检查(p2+dim)
0x00000001141c9b04: b.hs <range_check_fail>          ; 越界跳转
0x00000001141c9b08: cmp w0, w12                      ; 边界检查(p1)
0x00000001141c9b0c: b.hs <range_check_fail>          ; 越界跳转
0x00000001141c9b10: cmp x14, x16                     ; 边界检查(p1+dim)
0x00000001141c9b14: b.hs <range_check_fail>          ; 越界跳转
```

这里是外循环，对边界检查进行集中处理，降低了内循环对数组边界的重复检查，此外，也用到了`b.hs`指令对热点执行代码进行展开

**综上所述，我们可以看到JIT的编译器具有较高层次的优化，虽然没有在一些细节方面没有c的编译器优化的全面，但是能够提升性能的主要优化都是做到了的，这也就导致在最终结果上java代码的执行速度接近c代码在O2、O3优化下的执行速度。**

一旦关闭了JIT实时编译，全部代码由解释器执行，那么java程序的计算性能会大幅下降。利用`-Xint`执行选项禁用JIT，得到如下结果：

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-30 14.15.13.png" alt="截屏2025-03-30 14.15.13" style="zoom:50%;" />

可见，没有JIT优化的java运行速度远比c要更为缓慢，而有了JIT优化后，速度有着极大幅度的提升

## 5 多线程测试

### 5.1 OpenMP & concurrent

为了追求更高的性能表现和测试java和c在更多领域的区别，我还测试了java和c的多线程表现。java的多线程实现采用了`java.util.concurrent`包中的内容，在deepseek的帮助下进行函数实现，以下以int计算函数为例：

```java
public static int calculateVectorInt(int[] data, int N, int dim) {
    VectorAccumulator acc = new VectorAccumulator();
    ExecutorService executor = Executors.newFixedThreadPool(THREADS);

    for (int i = 1; i <= N/2; i++) {
        final int currentI = i;
        executor.execute(() -> {
            int p1 = (2 * currentI - 2) * dim;
            int p2 = (2 * currentI - 1) * dim;
            int localSum = 0;

            for (int j = 0; j < dim; j++) {
                localSum += data[p1 + j] * data[p2 + j];
            }

            acc.add(localSum);
        });
    }

    executor.shutdown();
    try {
        executor.awaitTermination(1, TimeUnit.MINUTES);
    } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
    }

    return acc.getIntSum();
}
```

而c语言的多线程实现依赖于openMP，以下也是int计算函数的实现，和原始函数的区别仅在于多了标识多线程计算的pragma：

```c
omp_set_num_threads(num_threads);//设置线程数
......
int calculate_vector_int(int* data, int N, int dim) {
    int sum = 0;
    if (!data) {
        printf("empty data passing\n");
        return sum;
    }

    #pragma omp parallel for reduction(+:sum) schedule(guided)
    for (int i = 1; i <= N/2 ; i++) {
        int offset1 = (2 * i - 2) * dim;
        int offset2 = (2 * i - 1) * dim;
        int local_sum = 0;
        
        for (int j = 0; j < dim; j++) {
            local_sum += data[offset1 + j] * data[offset2 + j];
        }
        
        sum += local_sum;
    }
    return sum;
}
```

### 5.2 实验1:数据量较大时数据类型变化对多线程速度的影响

据此，得到两个新的可执行文件加入数据量较大时数据类型变化的测试，两个多线程程序均选用4个线程数，而c代码的多线程程序在编译时也使用了Ofast优化。因为相对时间差太大的关系，为了更直观地感受多线程运算的速度，我将c代码的无优化和O1优化测试程序从测试中删除。结果如下图：

<img src="/Users/yun/Desktop/C_project2/截屏2025-03-29 23.52.37.png" alt="截屏2025-03-29 23.52.37" style="zoom:100%;" />

橙色为java多线程，棕色为c多线程。**可以看到java多线程在整型计算部分性能相比单线程有所提升，但总体提升较小，但在计算浮点型时速度有了接近一倍多的提升；而ofast优化下的c多线程则相比于单线程的ofast优化在各个数据类型的测试中均有将近一倍以上的提升，**达到了全部测试中的最快速度。到了这里，c代码的优势终于有所凸显。

### 5.3 实验2:向量数量增加下多线程的速度对比

此外，根据以上几个代码，我又测试了仅N增长下的性能对比：

![截屏2025-03-30 00.06.11](/Users/yun/Desktop/C_project2/截屏2025-03-30 00.06.11.png)

其中最值得注意的是，java多线程在N数量较小时甚至速度比java单线程还慢，但在N较大时则超过了Ofast优化的c代码。这其中原因可能在于

- **分配线程时存在一定的固有开销，这一部分开销占到总计算时间的一定比例，影响了计算**
- **多线程下，JIT识别热点代码的频次也是分别计算的，这就导致多线程相比单线程在数据量增长时会更慢地进入JIT编译热点代码进行加速的阶段，因为线程增加也就意味着每个线程内的函数调用更少，使得达到被JIT识别成热代码所需的数据量更大**

### 5.4 实验3:线程数量变化对多线程性能的影响

以上部分是在双方线程数均为4的情况下测试的，那么当我们改变线程数量时，是否会影响性能呢？以下是设置线程数从1到32增加，在数据类型为float，N=10000，dim=5000的情况下，分别对java和c多线程程序进行测试得到的结果。左图的纵轴为时间，右图的纵轴为多线程时间对比单线程时间的比值。

![截屏2025-03-30 01.08.44](/Users/yun/Desktop/C_project2/截屏2025-03-30 01.08.44.png)

可以观察到以下现象：

1. c多线程的执行速度要快于java多线程
2. java多线程在4～8线程左右达到最快，随后随着线程数量增多，性能反而略有下降
3. 而c在线程数量增多的情况下也在4～8线程左右达到最快，但是随后在线程增多的情况下虽有一定变缓趋势，但走势总体相对平稳
4. 仅比对右图，红色线基本一直处于蓝色线下方，表明java虽然计算速度较慢，但多线程对比自身单线程提升的速度要更快

由于本地测试环境的物理核心数为8(6+2)，所以图线表明速度最快的位置在4～8符合预测事实。根据图线，总体来说多线程的提升使得单线程的速度是多线程的0.25~0.35左右，java性能提升约2.85倍左右，c性能提升约4倍左右

**然而，随着线程数增加，按照道理速度应该随着线程数翻倍而一起翻倍，但在线程数超过核心数的时候，这个理论就不符合实际测试的结果了。这其中原因大概是由于当线程数超过物理核心数时，多个逻辑线程会共享同一个物理核心的计算资源（ALU、FPU、执行端口、缓存等）。随着线程数增加，系统吞吐量逐渐达到计算资源和内存带宽的上限，之后，新增线程的计算任务需要排队等待资源，无法带来实质性能提升。当线程数过多时，整体性能反而可能下降。这也就表现为图片中后期速度放缓的现象。**

## 6 总结与反思

### 6.1 实验结论

本次实验通过对比Java和C语言在向量点积计算上的性能表现，分析了两种语言不同编译器的优化和多线程的效果差异。

**在性能与数据量的关系方面**，总体来看在数据量较大的情况下，两种语言在不同优化级别和数据类型的性能比较大体如下：

​									int, float,double：C-Ofast > C-O3 = C-O2 = java > C-O1 > C-O0

​									short,unsigned char：C-Ofast > C-O3 = C-O2 > java > C-O1 > C-O0

​									float,double > int, short,unsigned char

此外还发现发现Java在小数据量时表现不佳，这主要源于JIT编译器需要一定的预热时间才能识别并优化热点代码，而随着数据量增大，JIT的优化效果逐渐显现，最终性能如上所示，可以接近C语言的O2/O3优化水平。相比之下，C语言在各种数据规模下都保持稳定的性能表现，特别是在使用较高优化级别时，这得益于其静态编译的特性。

**在编译优化效果方面**，我们基于汇编代码详细分析了C语言从O0到Ofast各级优化的具体改进。O1优化的主要贡献是减少栈操作，增加寄存器使用，并简化控制流；O2优化则引入了SIMD并行计算，这对较小数据类型如short和char的性能提升尤为显著，因为这些类型可以在NEON的128位寄存器中容纳更多元素；O3优化相比O2提升有限，主要做了一些较小的循环展开，并重新分配了指令的位置，最终性能提升效果不明显；而Ofast优化特别在浮点运算方面有很大的提升，因为它使用了更高效的乘加指令来实现优化。Java的JIT优化综合了类似O1和O2的主要优化策略，因而在较大数据量下总体性能可以和O2、O3优化媲美，但在浮点运算上没有采用更高效的指令，因而在这方面效果不如Ofast。

**数据类型的选择对性能有显著影响**。实验显示，位宽较小的数据类型在C语言O2/O3优化下能获得最大性能提升，这是因为SIMD指令可以同时处理更多数据元素。例如，一个128位寄存器可以容纳8个short或16个char，但只能容纳4个int。Java对不同数据类型的性能差异较小，因为JIT优化机制中并没有根据数据类型进行特化计算。

**在多线程性能方面**，经过测试发现，C语言的OpenMP具有明显优势，结合了Ofast优化后更是达到了极致的性能效果。两种语言的最佳线程数都在4-8之间，这与测试环境的物理核心数相符，并且最大性能提升在2.8~4倍左右。有趣的是，Java多线程在小数据量时反而比单线程更慢，可能是由于线程创建和管理开销超过了并行计算带来的收益。当线程数超过物理核心数后，两种语言的性能提升都趋于平缓，可能因为过多的线程会导致内存带宽和计算资源占用达到饱和。

基于这些发现，我们可以总结出，**在开发效率、安全性能更重要的场景，java在数据量较大情况下的实际计算效果也并没有弱于不对计算做特别优化的C语言，具有较为良好的实际效果；但如果要追求极致计算性能，特别是涉及小数据类型或浮点运算时，C语言配合最高级别的编译优化和多线程是最佳选择。**

### 6.2 实验局限性

本次实验虽然通过大量测试和分析，取得很多结果，然而也存在较多不足之处：

1. 在测量程序计算时间时，具体速度可能和当时受电脑当时运行状态的一定影响，然而实验并没有给出方案具体解决这个问题，可能导致速度存在不精确性，也导致了没有办法去测试数据量更小的情况下的运行速度，因为可能存在较大的波动
2. 全部测试是基于ARM64架构的苹果电脑，并没有测试在其他架构和硬件下的环境中的性能表现，因而实验的分析和结论不一定具有通用性，而是仅仅在特定环境下具备一定的可信度
3. 虽然在汇编代码层面分析了原因，但是没有找到在运行期间对内存占用和任务调度进行实时分析的办法，导致并没能深入从内存角度分析两种语言运行的差异，也没有分析通过分析CPI来具体证明汇编的优化是否真的起效果了
4. 虽然保留了运算结果但是并没有仔细对比各个优化级别下的结果是否完全一致，因而没有进一步分析高性能表现有没有可能导致精度损失问题，而这一点又何汇编层面的计算逻辑有什么关联

## 7 引用

1. AArch64指令集 profile https://developer.arm.com/documentation/ddi0487/latest
2. JIT 原理 https://developer.aliyun.com/article/1139159
3. JITwatch使用 https://cloud.tencent.com/developer/article/1170384
4. 项目架构画图网站 https://boardmix.cn/app/editor/F7BPjerCClW-UlhpLVqOQQ
5. 前人项目思路参考 https://github.com/HaibinLai/CS205-CPP-Programing-Project/tree/main/Project2